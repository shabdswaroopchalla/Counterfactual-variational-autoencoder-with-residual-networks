{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45a3aea8",
   "metadata": {
    "papermill": {
     "duration": 0.008326,
     "end_time": "2025-04-26T13:46:13.652095",
     "exception": false,
     "start_time": "2025-04-26T13:46:13.643769",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# DataLoader and Dataset Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3c609dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-26T13:46:13.667986Z",
     "iopub.status.busy": "2025-04-26T13:46:13.667678Z",
     "iopub.status.idle": "2025-04-26T13:46:17.194913Z",
     "shell.execute_reply": "2025-04-26T13:46:17.193889Z"
    },
    "papermill": {
     "duration": 3.537144,
     "end_time": "2025-04-26T13:46:17.196994",
     "exception": false,
     "start_time": "2025-04-26T13:46:13.659850",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU name: Tesla P100-PCIE-16GB\n",
      "Memory allocated: 0.0 MB\n",
      "Memory cached: 0.0 MB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Print device info\n",
    "print(\"Using device:\", device)\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU name:\", torch.cuda.get_device_name(0))\n",
    "    print(\"Memory allocated:\", torch.cuda.memory_allocated(0) / 1024**2, \"MB\")\n",
    "    print(\"Memory cached:\", torch.cuda.memory_reserved(0) / 1024**2, \"MB\")\n",
    "else:\n",
    "    print(\"No GPU found. Using CPU.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49b685f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-26T13:46:17.221047Z",
     "iopub.status.busy": "2025-04-26T13:46:17.220435Z",
     "iopub.status.idle": "2025-04-26T13:46:25.633859Z",
     "shell.execute_reply": "2025-04-26T13:46:25.632655Z"
    },
    "papermill": {
     "duration": 8.424083,
     "end_time": "2025-04-26T13:46:25.635436",
     "exception": false,
     "start_time": "2025-04-26T13:46:17.211353",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomly sampling 30000 images from 202599 total.\n",
      "\n",
      "Testing DataLoader with separate annotation files...\n",
      "\n",
      "Batch 0:\n",
      "  Images shape:           torch.Size([32, 3, 64, 64]), dtype: torch.float32\n",
      "  Full Annotations shape: torch.Size([32, 40]), dtype: torch.float32\n",
      "  Causal Annotations shape:torch.Size([32, 4]), dtype: torch.float32\n",
      "  Image range:            min=0.0000, max=1.0000\n",
      "\n",
      "Batch 1:\n",
      "  Images shape:           torch.Size([32, 3, 64, 64]), dtype: torch.float32\n",
      "  Full Annotations shape: torch.Size([32, 40]), dtype: torch.float32\n",
      "  Causal Annotations shape:torch.Size([32, 4]), dtype: torch.float32\n",
      "  Image range:            min=0.0000, max=1.0000\n",
      "\n",
      "Batch 2:\n",
      "  Images shape:           torch.Size([32, 3, 64, 64]), dtype: torch.float32\n",
      "  Full Annotations shape: torch.Size([32, 40]), dtype: torch.float32\n",
      "  Causal Annotations shape:torch.Size([32, 4]), dtype: torch.float32\n",
      "  Image range:            min=0.0000, max=1.0000\n",
      "\n",
      "DataLoader test finished.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image, ImageFile\n",
    "import random\n",
    "import PIL # Ensure PIL is imported if needed for error handling\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "class CelebAFromSeparateAnnotationsDataset(Dataset):\n",
    "    \"\"\"\n",
    "    CelebA Dataset loading images and annotations from two separate files\n",
    "    (one full, one causal), assuming they correspond row-by-row based on\n",
    "    a shared identifier (e.g., filename in the first column).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, image_dir, full_annotations_file, causal_annotations_file,\n",
    "                 transform=None, num_samples=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_dir (string): Directory with all the images.\n",
    "            full_annotations_file (string): Path to the file with ALL annotations.\n",
    "                                            Assumes first column is image filename.\n",
    "            causal_annotations_file (string): Path to the file with ONLY causal annotations.\n",
    "                                              Assumes first column is image filename.\n",
    "            transform (callable, optional): Optional transform to be applied on the image sample.\n",
    "            num_samples (int, optional): Number of images to randomly sample. If None, use all images.\n",
    "        \"\"\"\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        try:\n",
    "            # Load both annotation files\n",
    "            self.full_annotations_df = self._read_annotation_file(full_annotations_file)\n",
    "            self.causal_annotations_df = self._read_annotation_file(causal_annotations_file)\n",
    "        except FileNotFoundError as e:\n",
    "            raise FileNotFoundError(f\"Annotation file error: {e}\")\n",
    "        except ValueError as e:\n",
    "             raise ValueError(f\"Annotation file error: {e}\")\n",
    "\n",
    "\n",
    "        # --- Crucial Assumption & Verification ---\n",
    "        # Assume the first column is the filename/identifier in both files\n",
    "        self.filename_col_name = self.full_annotations_df.columns[0]\n",
    "        if self.causal_annotations_df.columns[0] != self.filename_col_name:\n",
    "            raise ValueError(f\"Filename column mismatch: '{self.filename_col_name}' vs \"\n",
    "                             f\"'{self.causal_annotations_df.columns[0]}'\")\n",
    "\n",
    "        # Verify that the files have the same number of rows (important for direct indexing)\n",
    "        if len(self.full_annotations_df) != len(self.causal_annotations_df):\n",
    "            print(f\"Warning: Annotation files have different lengths \"\n",
    "                  f\"({len(self.full_annotations_df)} vs {len(self.causal_annotations_df)}). \"\n",
    "                  f\"This might lead to errors unless filenames match perfectly.\")\n",
    "            # Consider adding a merge or stricter check here if lengths differ significantly\n",
    "            # For now, we proceed assuming filenames will align for the sampled indices\n",
    "\n",
    "        # You could optionally add a check here that the filename columns themselves match\n",
    "        # if not self.full_annotations_df[self.filename_col_name].equals(self.causal_annotations_df[self.filename_col_name]):\n",
    "        #    print(\"Warning: Filename columns do not match exactly between files.\")\n",
    "\n",
    "        # --- Sampling ---\n",
    "        total_images = len(self.full_annotations_df) # Sample based on the full annotations file length\n",
    "        if num_samples is None:\n",
    "            self.num_samples = total_images\n",
    "            self.sampled_indices = list(range(total_images)) # Use all indices\n",
    "            print(f\"Using all {self.num_samples} images.\")\n",
    "        else:\n",
    "            self.num_samples = min(num_samples, total_images)\n",
    "            print(f\"Randomly sampling {self.num_samples} images from {total_images} total.\")\n",
    "            self.sampled_indices = random.sample(range(total_images), self.num_samples)\n",
    "\n",
    "    def _read_annotation_file(self, filepath):\n",
    "        \"\"\"Helper function to read CSV or Excel.\"\"\"\n",
    "        if filepath.endswith('.xlsx') or filepath.endswith('.xls'):\n",
    "            return pd.read_excel(filepath)\n",
    "            \n",
    "        elif filepath.endswith('.csv'):\n",
    "            return pd.read_csv(filepath)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported annotations file format. Use .csv, .xlsx, or .xls\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        if not isinstance(idx, int):\n",
    "             raise TypeError(f\"Index must be an integer. Got: {type(idx)}\")\n",
    "        if not (0 <= idx < self.num_samples):\n",
    "             raise IndexError(f\"Index {idx} out of bounds for dataset with length {self.num_samples}\")\n",
    "\n",
    "        # Use the pre-sampled index to access the correct row in the *full* annotations df\n",
    "        original_idx = self.sampled_indices[idx]\n",
    "        full_row_data = self.full_annotations_df.iloc[original_idx]\n",
    "\n",
    "        img_filename = full_row_data.iloc[0] # Get filename from the full annotations row\n",
    "        img_name = os.path.join(self.image_dir, img_filename)\n",
    "\n",
    "        # --- Find the corresponding row in the causal annotations file ---\n",
    "        # This assumes filenames are unique identifiers and present in both files for the sampled indices\n",
    "        causal_row_data = self.causal_annotations_df[\n",
    "            self.causal_annotations_df[self.filename_col_name] == img_filename\n",
    "        ]\n",
    "\n",
    "        if causal_row_data.empty:\n",
    "             print(f\"Warning: Filename '{img_filename}' found in full annotations (idx {original_idx}) \"\n",
    "                   f\"but not found in causal annotations file. Returning None.\")\n",
    "             return None, None, None # Need to handle this in collate_fn\n",
    "\n",
    "        # Ensure only one matching row was found\n",
    "        if len(causal_row_data) > 1:\n",
    "            print(f\"Warning: Multiple rows found for filename '{img_filename}' in causal annotations. \"\n",
    "                  f\"Using the first match. Returning None.\")\n",
    "            return None, None, None # Treat as error for now\n",
    "\n",
    "\n",
    "        # --- Load Image ---\n",
    "        try:\n",
    "            image = Image.open(img_name).convert('RGB')\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: Image file not found: {img_name} (Index: {idx}, Original Index: {original_idx}). Returning None.\")\n",
    "            return None, None, None\n",
    "        except (PIL.UnidentifiedImageError, OSError) as e:\n",
    "            print(f\"Warning: Error loading image {img_name} (Index: {idx}, Original Index: {original_idx}): {e}. Returning None.\")\n",
    "            return None, None, None\n",
    "\n",
    "        # --- Extract Annotations ---\n",
    "        try:\n",
    "            # Extract ALL annotations (from 2nd column onwards in full df row)\n",
    "            all_annotations_np = full_row_data.iloc[1:].values.astype('float32')\n",
    "\n",
    "            # Extract CAUSAL annotations (from 2nd column onwards in the matched causal df row)\n",
    "            # Use .iloc[0] because filtering returns a DataFrame, we need the Series/row\n",
    "            causal_annotations_np = causal_row_data.iloc[0, 1:].values.astype('float32')\n",
    "        except (ValueError, IndexError) as e:\n",
    "             print(f\"Warning: Error converting/extracting annotations for image {img_filename} (Index: {idx}, Orig Idx: {original_idx}): {e}. Returning None.\")\n",
    "             return None, None, None\n",
    "\n",
    "        # Apply image transformation\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        else:\n",
    "            image = transforms.ToTensor()(image) # Basic conversion if no transform\n",
    "\n",
    "        # Convert numpy arrays to tensors\n",
    "        all_annotations_tensor = torch.from_numpy(all_annotations_np)\n",
    "        causal_annotations_tensor = torch.from_numpy(causal_annotations_np)\n",
    "\n",
    "        # --- Final Check (Optional but Recommended) ---\n",
    "        # Add checks here if needed to ensure dimensions match expected values (e.g., 40 and 4)\n",
    "        # if all_annotations_tensor.shape[0] != 40: print(\"Warning: Full annotation dim mismatch\")\n",
    "        # if causal_annotations_tensor.shape[0] != 4: print(\"Warning: Causal annotation dim mismatch\")\n",
    "\n",
    "        return image, all_annotations_tensor, causal_annotations_tensor\n",
    "\n",
    "# --- Example Usage ---\n",
    "\n",
    "# Define your image transformations\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(), # Scales images to [0, 1]\n",
    "])\n",
    "\n",
    "# Define file paths (MAKE SURE THESE ARE CORRECT)\n",
    "image_directory = \"/kaggle/input/celeba-dataset/img_align_celeba/img_align_celeba\"\n",
    "full_annotations_filepath = \"/kaggle/input/annotation/updated_Annotations_Complete.csv\" # File with all 40 attributes\n",
    "causal_annotations_filepath = \"/kaggle/input/annotation/updated_Annotations_Causal.csv\" # File with only the 4 causal attributes\n",
    "\n",
    "# Instantiate the dataset\n",
    "try:\n",
    "    separate_files_dataset = CelebAFromSeparateAnnotationsDataset(\n",
    "        image_dir=image_directory,\n",
    "        full_annotations_file=full_annotations_filepath,\n",
    "        causal_annotations_file=causal_annotations_filepath,\n",
    "        transform=data_transform,\n",
    "        num_samples=30000 # Or None to use all\n",
    "    )\n",
    "\n",
    "    # Re-use the safe collate function\n",
    "    def safe_collate(batch):\n",
    "        batch = [item for item in batch if item is not None and item[0] is not None]\n",
    "        if not batch: return None, None, None\n",
    "        images = torch.stack([item[0] for item in batch])\n",
    "        full_annotations = torch.stack([item[1] for item in batch])\n",
    "        causal_annotations = torch.stack([item[2] for item in batch])\n",
    "        return images, full_annotations, causal_annotations\n",
    "\n",
    "    # Create DataLoader\n",
    "    batch_size = 32\n",
    "    dataloader = DataLoader(\n",
    "        separate_files_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=2,\n",
    "        collate_fn=safe_collate,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    # --- Test the DataLoader ---\n",
    "    print(f\"\\nTesting DataLoader with separate annotation files...\")\n",
    "    num_batches_to_test = 3\n",
    "    for i, batch_data in enumerate(dataloader):\n",
    "        if i >= num_batches_to_test: break\n",
    "        if batch_data[0] is None:\n",
    "            print(f\"Batch {i}: Skipped (collate fn returned None)\")\n",
    "            continue\n",
    "\n",
    "        images_batch, full_annotations_batch, causal_annotations_batch = batch_data\n",
    "        print(f\"\\nBatch {i}:\")\n",
    "        print(f\"  Images shape:           {images_batch.shape}, dtype: {images_batch.dtype}\")\n",
    "        # Verify dimensions match expected full/causal counts\n",
    "        print(f\"  Full Annotations shape: {full_annotations_batch.shape}, dtype: {full_annotations_batch.dtype}\") # Expect [B, 40]\n",
    "        print(f\"  Causal Annotations shape:{causal_annotations_batch.shape}, dtype: {causal_annotations_batch.dtype}\") # Expect [B, 4]\n",
    "        print(f\"  Image range:            min={images_batch.min().item():.4f}, max={images_batch.max().item():.4f}\")\n",
    "\n",
    "    print(\"\\nDataLoader test finished.\")\n",
    "\n",
    "except (ValueError, FileNotFoundError, IndexError) as e:\n",
    "     print(f\"Error initializing dataset: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d95c30c",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-26T13:46:25.651494Z",
     "iopub.status.busy": "2025-04-26T13:46:25.651170Z",
     "iopub.status.idle": "2025-04-26T13:46:25.659319Z",
     "shell.execute_reply": "2025-04-26T13:46:25.658491Z"
    },
    "papermill": {
     "duration": 0.017587,
     "end_time": "2025-04-26T13:46:25.660670",
     "exception": false,
     "start_time": "2025-04-26T13:46:25.643083",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image, ImageFile\n",
    "import random\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "class CelebADataset(Dataset):\n",
    "    \"\"\"CelebA Dataset with image and annotations from Excel, randomly sampled.\"\"\"\n",
    "\n",
    "    def __init__(self, image_dir, annotations_file, transform=None, num_samples=30000):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_dir (string): Directory with all the images.\n",
    "            annotations_file (string): Path to the CSV/Excel file with annotations.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "            num_samples (int, optional): Number of images to randomly sample.\n",
    "        \"\"\"\n",
    "        self.image_dir = image_dir\n",
    "        self.annotations = pd.read_excel(annotations_file)  # or pd.read_csv\n",
    "        self.transform = transform\n",
    "        self.num_samples = min(num_samples, len(self.annotations))  # Ensure we don't sample more than available\n",
    "        self.sampled_indices = random.sample(range(len(self.annotations)), self.num_samples) # Store the randomly sampled indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples  # Return the number of sampled images\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        # Use the pre-sampled indices to access the correct rows in the annotation file\n",
    "        original_idx = self.sampled_indices[idx]\n",
    "        img_name = os.path.join(self.image_dir, self.annotations.iloc[original_idx, 0])  # Assuming filename is the first column\n",
    "\n",
    "        try:\n",
    "            image = Image.open(img_name).convert('RGB')  # Ensure RGB format\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: Image file not found: {img_name}\")\n",
    "            return None, None  # Return None if the image is not found\n",
    "        except PIL.UnidentifiedImageError:\n",
    "            print(f\"Warning: Could not open or read image file (corrupted?): {img_name}\")\n",
    "            return None, None\n",
    "        except OSError as e:  # Catch other potential image opening errors\n",
    "            print(f\"Warning: Could not open image due to OSError: {e}\")\n",
    "            return None, None\n",
    "\n",
    "        # Assuming annotations are in the remaining columns.  Adjust indices if needed.\n",
    "        annotations = self.annotations.iloc[original_idx, 1:].values.astype('float32')  # Convert annotations to numpy array of float32\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        annotations = torch.tensor(annotations)  # convert numpy array to tensor\n",
    "\n",
    "        return (image, annotations)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62eff16",
   "metadata": {
    "papermill": {
     "duration": 0.007036,
     "end_time": "2025-04-26T13:46:25.674853",
     "exception": false,
     "start_time": "2025-04-26T13:46:25.667817",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Convolutional Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f548103a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-26T13:46:25.690110Z",
     "iopub.status.busy": "2025-04-26T13:46:25.689900Z",
     "iopub.status.idle": "2025-04-26T13:46:25.697600Z",
     "shell.execute_reply": "2025-04-26T13:46:25.696928Z"
    },
    "papermill": {
     "duration": 0.016842,
     "end_time": "2025-04-26T13:46:25.698834",
     "exception": false,
     "start_time": "2025-04-26T13:46:25.681992",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# =============================================================================\n",
    "# Placeholder for the ut.gaussian_parameters function\n",
    "# This assumes it splits the channels (dim=1) into mean and logvar/softplus(var)\n",
    "# Add a small epsilon to variance for numerical stability.\n",
    "# =============================================================================\n",
    "def gaussian_parameters(x, dim=1):\n",
    "    \"\"\"\n",
    "    Splits the tensor x along the specified dimension into two halves,\n",
    "    interpreting them as parameters (mean and variance) for a Gaussian.\n",
    "    Applies Softplus to the variance part to ensure positivity.\n",
    "\n",
    "    Args:\n",
    "        x (torch.Tensor): Input tensor.\n",
    "        dim (int): Dimension along which to split.\n",
    "\n",
    "    Returns:\n",
    "        tuple[torch.Tensor, torch.Tensor]: Mean and Variance tensors.\n",
    "    \"\"\"\n",
    "    n_channels = x.size(dim)\n",
    "    if n_channels % 2 != 0:\n",
    "        raise ValueError(f\"Number of channels ({n_channels}) must be even to split into mu and var.\")\n",
    "    mu, var_input = torch.chunk(x, 2, dim=dim)\n",
    "    # Ensure variance is positive and numerically stable\n",
    "    var = F.softplus(var_input) + 1e-6\n",
    "    return mu, var\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "class ConvEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolutional Encoder for 64x64x3 images.\n",
    "    Uses a specific convolutional stack (`conv6`) to produce\n",
    "    parameters (mean and variance) for a latent distribution.\n",
    "\n",
    "    Input: Batch of images, shape [B, 3, 64, 64]\n",
    "    Output: mu, var (mean and variance tensors), each shape [B, z_dim]\n",
    "            where z_dim is determined by the final layer output channels / 2.\n",
    "    \"\"\"\n",
    "    def __init__(self, z_dim_target=64):\n",
    "        super().__init__()\n",
    "\n",
    "        # Define the convolutional stack\n",
    "        # Output channels of the second-to-last conv layer determine features\n",
    "        # Final conv layer outputs 2 * z_dim channels (for mu and var)\n",
    "        final_out_channels = 2 * z_dim_target\n",
    "\n",
    "        self.conv6 = nn.Sequential(\n",
    "            # Input: B x 3 x 64 x 64\n",
    "            nn.Conv2d(3, 32, 4, 2, 1),  # B x 32 x 32 x 32\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(32, 32, 4, 2, 1), # B x 32 x 16 x 16\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(32, 64, 4, 2, 1), # B x 64 x 8 x 8\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(64, 64, 4, 2, 1), # B x 64 x 4 x 4\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(64, 64, 4, 2, 1), # B x 64 x 2 x 2\n",
    "            nn.ReLU(True),\n",
    "            # --- Correction: Added padding=1 to handle 2x2 input with 4x4 kernel ---\n",
    "            nn.Conv2d(64, 256, 4, 1, padding=1), # B x 256 x 1 x 1\n",
    "            nn.ReLU(True),\n",
    "            # --- Final 1x1 convolution to get desired output channels ---\n",
    "            nn.Conv2d(256, final_out_channels , 1) # B x (2*z_dim) x 1 x 1\n",
    "        )\n",
    "\n",
    "        # Store the latent dimension\n",
    "        self.z_dim = z_dim_target\n",
    "        print(f\"ConvEncoder initialized for z_dim = {self.z_dim}\")\n",
    "        print(f\"Expected input shape: [B, 3, 64, 64]\")\n",
    "        print(f\"Output shape: mu=[B, {self.z_dim}], var=[B, {self.z_dim}]\")\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Encodes the input image batch x into latent distribution parameters.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input image batch, shape [B, 3, 64, 64].\n",
    "\n",
    "        Returns:\n",
    "            tuple[torch.Tensor, torch.Tensor]: Mean and Variance tensors (mu, var),\n",
    "                                                each with shape [B, z_dim].\n",
    "        \"\"\"\n",
    "        # Pass input through the convolutional layers\n",
    "        features = self.conv6(x) # Shape: B x (2*z_dim) x 1 x 1\n",
    "\n",
    "        # Extract mean and variance parameters\n",
    "        # Assumes gaussian_parameters splits along dim=1 (channels)\n",
    "        # and handles the (1x1) spatial dimensions appropriately (e.g., squeezes)\n",
    "        mu, var = gaussian_parameters(features, dim=1) # mu/var shape: [B, z_dim, 1, 1]\n",
    "\n",
    "        # Remove trailing spatial dimensions if they exist\n",
    "        mu = mu.squeeze(-1).squeeze(-1) # Shape: [B, z_dim]\n",
    "        var = var.squeeze(-1).squeeze(-1) # Shape: [B, z_dim]\n",
    "\n",
    "        return mu, var"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575f6365",
   "metadata": {
    "papermill": {
     "duration": 0.006934,
     "end_time": "2025-04-26T13:46:25.712819",
     "exception": false,
     "start_time": "2025-04-26T13:46:25.705885",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Conv_Conditional_Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f378918",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-26T13:46:25.727622Z",
     "iopub.status.busy": "2025-04-26T13:46:25.727414Z",
     "iopub.status.idle": "2025-04-26T13:46:25.735863Z",
     "shell.execute_reply": "2025-04-26T13:46:25.735206Z"
    },
    "papermill": {
     "duration": 0.017237,
     "end_time": "2025-04-26T13:46:25.737029",
     "exception": false,
     "start_time": "2025-04-26T13:46:25.719792",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ConvEncoderConditional(nn.Module):\n",
    "    \"\"\"\n",
    "    Conditional Convolutional Encoder for 64x64x3 images and annotations u.\n",
    "    Uses a convolutional stack to process the image, concatenates features\n",
    "    with annotations, and then uses linear layers to produce parameters\n",
    "    (mean and log-variance) for the conditional latent distribution q(z|x, u).\n",
    "\n",
    "    Input:\n",
    "        x: Batch of images, shape [B, 3, 64, 64]\n",
    "        u: Batch of annotations, shape [B, annotation_dim]\n",
    "    Output:\n",
    "        mu, logvar: Mean and log-variance tensors, each shape [B, z_dim]\n",
    "    \"\"\"\n",
    "    def __init__(self, z_dim_target=64, annotation_dim=4, hidden_fc_dim=128):\n",
    "        super().__init__()\n",
    "\n",
    "        # --- Convolutional part (processing image x) ---\n",
    "        # We'll stop before the final 1x1 conv that produced 2*z_dim channels\n",
    "        # The output here will be the flattened image features\n",
    "        image_feature_dim = 256 # Output channels of the last conv layer before flattening\n",
    "\n",
    "        # Define convolutional layers (excluding the original final 1x1 conv)\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            # Input: B x 3 x 64 x 64\n",
    "            nn.Conv2d(3, 32, 4, 2, 1),  # B x 32 x 32 x 32\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(32, 32, 4, 2, 1), # B x 32 x 16 x 16\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(32, 64, 4, 2, 1), # B x 64 x 8 x 8\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(64, 64, 4, 2, 1), # B x 64 x 4 x 4\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(64, 64, 4, 2, 1), # B x 64 x 2 x 2\n",
    "            nn.ReLU(True),\n",
    "            # --- Correction: Added padding=1 to handle 2x2 input with 4x4 kernel ---\n",
    "            nn.Conv2d(64, image_feature_dim, 4, 1, padding=1), # B x 256 x 1 x 1\n",
    "            nn.ReLU(True)\n",
    "            # Removed the final Conv2d(256, 2*z_dim, 1)\n",
    "        )\n",
    "\n",
    "        # --- Fully Connected part (combining image features and annotation u) ---\n",
    "        self.fc1 = nn.Linear(image_feature_dim + annotation_dim, hidden_fc_dim)\n",
    "\n",
    "        # Output layers for mean (mu) and log-variance (logvar)\n",
    "        self.fc_mu = nn.Linear(hidden_fc_dim, z_dim_target)\n",
    "        self.fc_logvar = nn.Linear(hidden_fc_dim, z_dim_target)\n",
    "\n",
    "        # Store dimensions\n",
    "        self.z_dim = z_dim_target\n",
    "        self.annotation_dim = annotation_dim\n",
    "        self.image_feature_dim = image_feature_dim\n",
    "\n",
    "        print(f\"ConvEncoderConditional initialized for z_dim = {self.z_dim}, annotation_dim = {self.annotation_dim}\")\n",
    "        print(f\"Expected input shapes: x=[B, 3, 64, 64], u=[B, {self.annotation_dim}]\")\n",
    "        print(f\"Output shape: mu=[B, {self.z_dim}], logvar=[B, {self.z_dim}]\")\n",
    "\n",
    "\n",
    "    def forward(self, x, u):\n",
    "        \"\"\"\n",
    "        Encodes the input image batch x and annotations u into latent distribution parameters.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input image batch, shape [B, 3, 64, 64].\n",
    "            u (torch.Tensor): Input annotation batch, shape [B, annotation_dim].\n",
    "\n",
    "        Returns:\n",
    "            tuple[torch.Tensor, torch.Tensor]: Mean and Log-variance tensors (mu, logvar),\n",
    "                                                each with shape [B, z_dim].\n",
    "        \"\"\"\n",
    "        if u.shape[0] != x.shape[0]:\n",
    "            raise ValueError(\"Batch size mismatch between image input x and annotation u.\")\n",
    "        if u.shape[1] != self.annotation_dim:\n",
    "             raise ValueError(f\"Input annotation dimension ({u.shape[1]}) does not match \"\n",
    "                              f\"encoder's expected annotation_dim ({self.annotation_dim})\")\n",
    "\n",
    "        # 1. Process image x through convolutional layers\n",
    "        img_features = self.conv_layers(x) # Shape: [B, image_feature_dim, 1, 1]\n",
    "\n",
    "        # 2. Flatten image features\n",
    "        img_features_flat = img_features.view(img_features.size(0), -1) # Shape: [B, image_feature_dim]\n",
    "\n",
    "        # 3. Concatenate flattened image features and annotations u\n",
    "        combined_features = torch.cat([img_features_flat, u], dim=1) # Shape: [B, image_feature_dim + annotation_dim]\n",
    "\n",
    "        # 4. Pass combined features through fully connected layers\n",
    "        hidden = F.relu(self.fc1(combined_features))\n",
    "\n",
    "        # 5. Compute mu and logvar\n",
    "        mu = self.fc_mu(hidden)           # Shape: [B, z_dim]\n",
    "        logvar = self.fc_logvar(hidden)   # Shape: [B, z_dim]\n",
    "\n",
    "        \n",
    "        return mu, logvar\n",
    "  \n",
    "def sample_z(mu, logvar):\n",
    "  \"\"\"\n",
    "  Samples z from the distribution N(mu, exp(logvar)) using the\n",
    "  reparameterization trick.\n",
    "\n",
    "  Args:\n",
    "      mu (torch.Tensor): Mean tensor from the encoder. Shape [B, z_dim].\n",
    "      logvar (torch.Tensor): Log-variance tensor from the encoder. Shape [B, z_dim].\n",
    "\n",
    "  Returns:\n",
    "      torch.Tensor: Sampled latent variable z. Shape [B, z_dim].\n",
    "  \"\"\"\n",
    "  std = torch.exp(0.5 * logvar)  # Calculate standard deviation sigma\n",
    "  eps = torch.randn_like(std)   # Sample epsilon ~ N(0, I)\n",
    "  z = mu + eps * std            # Apply the reparameterization trick\n",
    "  return z\n",
    "\n",
    "# --- Example Usage (assuming you have encoder output) ---\n",
    "# mu, logvar = encoder(x, u) # Get output from your ConvEncoderConditional\n",
    "# z = sample_z(mu, logvar)   # Sample z\n",
    "\n",
    "# Now 'z' is the latent vector you can pass to your decoder or classifier      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873c1733",
   "metadata": {
    "papermill": {
     "duration": 0.007185,
     "end_time": "2025-04-26T13:46:25.751303",
     "exception": false,
     "start_time": "2025-04-26T13:46:25.744118",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Resnet_Convolutional_Encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de531c02",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-26T13:46:25.766582Z",
     "iopub.status.busy": "2025-04-26T13:46:25.766351Z",
     "iopub.status.idle": "2025-04-26T13:46:25.783537Z",
     "shell.execute_reply": "2025-04-26T13:46:25.782800Z"
    },
    "papermill": {
     "duration": 0.02646,
     "end_time": "2025-04-26T13:46:25.784939",
     "exception": false,
     "start_time": "2025-04-26T13:46:25.758479",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DownsampleResBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A Residual Block for Downsampling using Conv2d with stride.\n",
    "    Applies BN and ReLU before convolutions (pre-activation style).\n",
    "    Handles changes in channels and spatial dimensions via the skip connection.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, stride=2, kernel_size=4, padding=1):\n",
    "        super().__init__()\n",
    "        self.stride = stride\n",
    "        self.channels_match = (in_channels == out_channels)\n",
    "\n",
    "        # Main (Residual) Path - F(x)\n",
    "        self.bn1 = nn.BatchNorm2d(in_channels)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        # Downsamples spatially if stride > 1, also changes channels\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size,\n",
    "                               stride=stride, padding=padding, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        # Second conv (kernel 3x3) refines features without changing size/channels\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "\n",
    "        # Skip Connection Path\n",
    "        self.needs_skip_proj = (stride > 1) or not self.channels_match\n",
    "        if self.needs_skip_proj:\n",
    "            # Use a single Conv2d layer for both downsampling (via stride)\n",
    "            # and channel matching (via out_channels).\n",
    "            self.skip_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, # Kernel 1 only changes channels\n",
    "                                       stride=stride, padding=0, bias=False)      # Stride handles downsampling\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # --- Skip Connection Path ---\n",
    "        identity = x\n",
    "        if self.needs_skip_proj:\n",
    "            identity = self.skip_conv(identity) # Downsample and/or change channels\n",
    "\n",
    "        # --- Main Path ---\n",
    "        out = self.bn1(x)\n",
    "        out = self.relu1(out)\n",
    "        out = self.conv1(out) # Downsample and change channels\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.conv2(out) # Refine features\n",
    "\n",
    "        # --- Add Skip Connection ---\n",
    "        out = out + identity # Element-wise addition\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ConvEncoderConditionalResNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Conditional Convolutional Encoder using DownsampleResBlocks for 64x64x3 images and annotations u.\n",
    "    Outputs parameters (mean and log-variance) for the conditional latent distribution q(epsilon|x, u).\n",
    "\n",
    "    Input:\n",
    "        x: Batch of images, shape [B, 3, 64, 64]\n",
    "        u: Batch of annotations, shape [B, annotation_dim] (e.g., 40)\n",
    "    Output:\n",
    "        mu, logvar: Mean and log-variance tensors, each shape [B, z_dim]\n",
    "    \"\"\"\n",
    "    def __init__(self, z_dim_target=64, annotation_dim=40, start_channels=16, dropout_p=0.3, hidden_fc_dim=128):\n",
    "        super().__init__()\n",
    "\n",
    "        self.z_dim = z_dim_target\n",
    "        self.annotation_dim = annotation_dim\n",
    "\n",
    "        # --- Initial Convolution ---\n",
    "        # Map input channels (3) to starting channels for ResBlocks\n",
    "        self.initial_conv = nn.Conv2d(3, start_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.initial_bn = nn.BatchNorm2d(start_channels)\n",
    "        self.initial_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # --- Sequence of Downsampling ResBlocks ---\n",
    "        layers = []\n",
    "        current_channels = start_channels\n",
    "        # Define channel progression (example, mirroring decoder approximately)\n",
    "        channel_config = [32, 64, 128, 256, 256] # Channels *after* each stage\n",
    "\n",
    "        # Stage 1: 64x64 -> 32x32\n",
    "        out_ch = channel_config[0] # 32\n",
    "        layers.append(DownsampleResBlock(current_channels, out_ch, stride=2, kernel_size=4, padding=1))\n",
    "        current_channels = out_ch\n",
    "\n",
    "        # Stage 2: 32x32 -> 16x16\n",
    "        out_ch = channel_config[1] # 64\n",
    "        layers.append(DownsampleResBlock(current_channels, out_ch, stride=2, kernel_size=4, padding=1))\n",
    "        current_channels = out_ch\n",
    "\n",
    "        # Stage 3: 16x16 -> 8x8\n",
    "        out_ch = channel_config[2] # 128\n",
    "        layers.append(DownsampleResBlock(current_channels, out_ch, stride=2, kernel_size=4, padding=1))\n",
    "        current_channels = out_ch\n",
    "\n",
    "        # Stage 4: 8x8 -> 4x4\n",
    "        out_ch = channel_config[3] # 256\n",
    "        layers.append(DownsampleResBlock(current_channels, out_ch, stride=2, kernel_size=4, padding=1))\n",
    "        current_channels = out_ch\n",
    "\n",
    "        # Stage 5: 4x4 -> 2x2\n",
    "        out_ch = channel_config[4] # 256 (keep same or increase)\n",
    "        layers.append(DownsampleResBlock(current_channels, out_ch, stride=2, kernel_size=4, padding=1))\n",
    "        current_channels = out_ch\n",
    "\n",
    "        # Stage 6: 2x2 -> 1x1 (Optional - can use AdaptiveAvgPool later)\n",
    "        # Instead of a ResBlock, use a Conv layer with kernel size matching input spatial dim\n",
    "        # This effectively acts like a flatten + linear projection\n",
    "        self.final_conv_collapse = nn.Conv2d(current_channels, current_channels, kernel_size=2, stride=1, padding=0)\n",
    "        # Alternatively, could use Adaptive Average Pooling:\n",
    "        # self.final_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        self.res_blocks = nn.Sequential(*layers)\n",
    "        self.final_bn_relu = nn.Sequential( # BN/ReLU after blocks, before final pooling/conv\n",
    "            nn.BatchNorm2d(current_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        # --- Fully Connected part (combining image features and annotation u) ---\n",
    "        self.image_feature_dim = current_channels # Features after pooling/collapsing\n",
    "        self.fc1 = nn.Linear(self.image_feature_dim + annotation_dim, hidden_fc_dim)\n",
    "        self.dropout = nn.Dropout(p=dropout_p)\n",
    "        self.fc_mu = nn.Linear(hidden_fc_dim, z_dim_target)\n",
    "        self.fc_logvar = nn.Linear(hidden_fc_dim, z_dim_target)\n",
    "\n",
    "        print(f\"ConvEncoderConditionalResNet initialized for z_dim = {self.z_dim}, annotation_dim = {self.annotation_dim}\")\n",
    "        print(f\"Output shape (mu, logvar): [B, {self.z_dim}]\")\n",
    "\n",
    "    def forward(self, x, u):\n",
    "        \"\"\"\n",
    "        Encodes the input image batch x and annotations u into latent distribution parameters.\n",
    "        \"\"\"\n",
    "        if u.shape[0] != x.shape[0]:\n",
    "            raise ValueError(\"Batch size mismatch between image input x and annotation u.\")\n",
    "        if u.shape[1] != self.annotation_dim:\n",
    "             raise ValueError(f\"Input annotation dimension ({u.shape[1]}) does not match \"\n",
    "                              f\"encoder's expected annotation_dim ({self.annotation_dim})\")\n",
    "\n",
    "        # 1. Initial Conv\n",
    "        out = self.initial_conv(x)\n",
    "        out = self.initial_bn(out)\n",
    "        out = self.initial_relu(out) # Shape: [B, start_channels, 64, 64]\n",
    "\n",
    "        # 2. Pass through ResBlocks\n",
    "        out = self.res_blocks(out) # Shape: [B, final_block_channels, 2, 2]\n",
    "\n",
    "        # 3. Final BN/ReLU and spatial collapse\n",
    "        out = self.final_bn_relu(out)\n",
    "        # out = self.final_pool(out) # Option 1: Use Pooling\n",
    "        out = self.final_conv_collapse(out) # Option 2: Use Conv\n",
    "        # Output shape after collapse/pool: [B, final_block_channels, 1, 1]\n",
    "\n",
    "        # 4. Flatten image features\n",
    "        img_features_flat = out.view(out.size(0), -1) # Shape: [B, final_block_channels]\n",
    "\n",
    "        # 5. Concatenate flattened image features and annotations u\n",
    "        combined_features = torch.cat([img_features_flat, u], dim=1)\n",
    "\n",
    "        # 6. Pass combined features through fully connected layers\n",
    "        hidden = F.relu(self.fc1(combined_features))\n",
    "        hidden_dropout = self.dropout(hidden) # Apply dropout\n",
    "\n",
    "        # 7. Compute mu and logvar\n",
    "        mu = self.fc_mu(hidden_dropout)\n",
    "        logvar = self.fc_logvar(hidden_dropout)\n",
    "\n",
    "        return mu, logvar\n",
    "\n",
    "\n",
    "# --- Example Usage ---\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# z_dim = 64\n",
    "# full_annotation_dim = 40\n",
    "\n",
    "# # Instantiate\n",
    "# encoder = ConvEncoderConditionalResNet(\n",
    "#     z_dim_target=z_dim,\n",
    "#     annotation_dim=full_annotation_dim,\n",
    "#     start_channels=32 # Example start channels\n",
    "# ).to(device)\n",
    "\n",
    "# # Create dummy inputs\n",
    "# dummy_x = torch.randn(4, 3, 64, 64).to(device)       # Batch size 4\n",
    "# dummy_u_full = torch.randn(4, full_annotation_dim).to(device)\n",
    "\n",
    "# # Forward pass\n",
    "# with torch.no_grad():\n",
    "#     mu_out, logvar_out = encoder(dummy_x, dummy_u_full)\n",
    "\n",
    "# print(f\"Output mu shape: {mu_out.shape}\")         # Should be [4, 64]\n",
    "# print(f\"Output logvar shape: {logvar_out.shape}\")   # Should be [4, 64]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178d4afd",
   "metadata": {
    "papermill": {
     "duration": 0.00771,
     "end_time": "2025-04-26T13:46:25.801275",
     "exception": false,
     "start_time": "2025-04-26T13:46:25.793565",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# causalclassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62bb1103",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-26T13:46:25.817954Z",
     "iopub.status.busy": "2025-04-26T13:46:25.817661Z",
     "iopub.status.idle": "2025-04-26T13:46:25.822602Z",
     "shell.execute_reply": "2025-04-26T13:46:25.821833Z"
    },
    "papermill": {
     "duration": 0.014982,
     "end_time": "2025-04-26T13:46:25.823954",
     "exception": false,
     "start_time": "2025-04-26T13:46:25.808972",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CausalMLPClassifier(nn.Module):\n",
    "    def __init__(self, latent_dim=64, annotation_dim=4, hidden_dim=256, output_dim=4):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            latent_dim (int): Dimension of the latent code from the encoder (z_dim), here 64.\n",
    "            annotation_dim (int): Dimension of the annotation vector u, e.g., 4.\n",
    "            hidden_dim (int): Number of neurons in the hidden layer.\n",
    "            output_dim (int): Dimension of the classifier output (typically same as annotation_dim).\n",
    "        \"\"\"\n",
    "        super(CausalMLPClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(latent_dim + annotation_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, z, u):\n",
    "        # Concatenate the latent code z and the annotation vector u along the feature dimension\n",
    "        x = torch.cat([z, u], dim=1)  # shape: [B, latent_dim + annotation_dim]\n",
    "        x = F.relu(self.fc1(x))\n",
    "        logits = self.fc2(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eead04da",
   "metadata": {
    "papermill": {
     "duration": 0.007763,
     "end_time": "2025-04-26T13:46:25.840067",
     "exception": false,
     "start_time": "2025-04-26T13:46:25.832304",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# causal Matrix A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6968dc03",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-26T13:46:25.856544Z",
     "iopub.status.busy": "2025-04-26T13:46:25.856140Z",
     "iopub.status.idle": "2025-04-26T13:46:25.861320Z",
     "shell.execute_reply": "2025-04-26T13:46:25.860430Z"
    },
    "papermill": {
     "duration": 0.014947,
     "end_time": "2025-04-26T13:46:25.862630",
     "exception": false,
     "start_time": "2025-04-26T13:46:25.847683",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def compute_causal_matrix(latent_features, u, classifier):\n",
    "    \"\"\"\n",
    "    Computes a 4x4 causal matrix A from classifier outputs.\n",
    "    \n",
    "    Args:\n",
    "        latent_features: Tensor of shape [batch_size, feature_dim] from the encoder.\n",
    "        u: Tensor of shape [batch_size, 4] representing the full annotation vector.\n",
    "        classifier: A model that takes (latent_features, u) and outputs logits for u (shape [batch_size, 4]).\n",
    "    \n",
    "    Returns:\n",
    "        A_estimated: Tensor of shape [4, 4] where each row i corresponds to the averaged Total Direct Effect\n",
    "                     when the i-th attribute is zeroed out.\n",
    "    \"\"\"\n",
    "    # Get factual predictions using the full annotation vector\n",
    "    factual_logits = classifier(latent_features, u)  # shape: [batch_size, 4]\n",
    "    \n",
    "    TDE_list = []  # To store the Total Direct Effect for each attribute\n",
    "    for i in range(u.shape[1]):  # Loop over 4 attributes\n",
    "        # Create a counterfactual annotation by zeroing out the i-th attribute\n",
    "        u_cf = u.clone()\n",
    "        u_cf[:, i] = 0.0\n",
    "        # Compute counterfactual predictions\n",
    "        cf_logits = classifier(latent_features, u_cf)\n",
    "        # Compute the Total Direct Effect for attribute i by averaging the difference over the batch\n",
    "        TDE_i = (factual_logits - cf_logits).mean(dim=0)  # shape: [4]\n",
    "        TDE_list.append(TDE_i)\n",
    "    \n",
    "    # Stack the TDE vectors along a new dimension to form the 4x4 causal matrix A\n",
    "    A_estimated = torch.stack(TDE_list, dim=0)  # shape: [4, 4]\n",
    "    return A_estimated\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1955f0",
   "metadata": {
    "papermill": {
     "duration": 0.007417,
     "end_time": "2025-04-26T13:46:25.878068",
     "exception": false,
     "start_time": "2025-04-26T13:46:25.870651",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Partitioning Into Es and Er"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b3ea78d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-26T13:46:25.893713Z",
     "iopub.status.busy": "2025-04-26T13:46:25.893401Z",
     "iopub.status.idle": "2025-04-26T13:46:25.897412Z",
     "shell.execute_reply": "2025-04-26T13:46:25.896565Z"
    },
    "papermill": {
     "duration": 0.013469,
     "end_time": "2025-04-26T13:46:25.898793",
     "exception": false,
     "start_time": "2025-04-26T13:46:25.885324",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def partition_latent(z, causal_dim=4):\n",
    "    \"\"\"\n",
    "    Splits the latent code z into es and er.\n",
    "    \n",
    "    Args:\n",
    "        z (torch.Tensor): The latent code with shape [B, z_dim].\n",
    "        causal_dim (int): Number of dimensions to allocate for the causal factors (es).\n",
    "        \n",
    "    Returns:\n",
    "        tuple[torch.Tensor, torch.Tensor]: es (causal factors) and er (residual information).\n",
    "    \"\"\"\n",
    "    es = z[:, :causal_dim]   # First `causal_dim` dimensions as es.\n",
    "    er = z[:, causal_dim:]   # Remaining dimensions as er.\n",
    "    return es, er"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b78dda1",
   "metadata": {
    "papermill": {
     "duration": 0.007125,
     "end_time": "2025-04-26T13:46:25.913124",
     "exception": false,
     "start_time": "2025-04-26T13:46:25.905999",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# MLP based Partitioner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "366c5163",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-26T13:46:25.928601Z",
     "iopub.status.busy": "2025-04-26T13:46:25.928316Z",
     "iopub.status.idle": "2025-04-26T13:46:25.933129Z",
     "shell.execute_reply": "2025-04-26T13:46:25.932312Z"
    },
    "papermill": {
     "duration": 0.014168,
     "end_time": "2025-04-26T13:46:25.934531",
     "exception": false,
     "start_time": "2025-04-26T13:46:25.920363",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LatentPartitioner(nn.Module):\n",
    "    \"\"\"\n",
    "    A learnable partitioner that takes the latent vector z\n",
    "    (shape [B, z_dim]) and outputs:\n",
    "      - es: causal factors (learned projection to causal_dim, e.g., 4)\n",
    "      - er: residual factors (learned projection to z_dim - causal_dim)\n",
    "    \"\"\"\n",
    "    def __init__(self, z_dim, causal_dim=4):\n",
    "        super(LatentPartitioner, self).__init__()\n",
    "        self.z_dim = z_dim\n",
    "        self.causal_dim = causal_dim\n",
    "        self.residual_dim = z_dim - causal_dim\n",
    "        \n",
    "        # Instead of hard slicing, use two linear layers to extract each part\n",
    "        self.es_layer = nn.Linear(z_dim, causal_dim)\n",
    "        self.er_layer = nn.Linear(z_dim, self.residual_dim)\n",
    "    \n",
    "    def forward(self, z):\n",
    "        es = self.es_layer(z)  # shape: [B, causal_dim]\n",
    "        er = self.er_layer(z)  # shape: [B, z_dim - causal_dim]\n",
    "        return es, er"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716d5a32",
   "metadata": {
    "papermill": {
     "duration": 0.007143,
     "end_time": "2025-04-26T13:46:25.948978",
     "exception": false,
     "start_time": "2025-04-26T13:46:25.941835",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Linear SCM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99901081",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-26T13:46:25.964719Z",
     "iopub.status.busy": "2025-04-26T13:46:25.964416Z",
     "iopub.status.idle": "2025-04-26T13:46:25.968693Z",
     "shell.execute_reply": "2025-04-26T13:46:25.968005Z"
    },
    "papermill": {
     "duration": 0.013658,
     "end_time": "2025-04-26T13:46:25.970033",
     "exception": false,
     "start_time": "2025-04-26T13:46:25.956375",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def linear_scm(es, A, epsilon=1e-3):\n",
    "    \"\"\"\n",
    "    Computes z_l = (I - Aᵀ + epsilon*I)⁻¹ * es using the pseudo-inverse for stability.\n",
    "    \n",
    "    Args:\n",
    "        es (torch.Tensor): Causal part of latent code, shape [B, n].\n",
    "        A (torch.Tensor): Estimated causal matrix, shape [n, n].\n",
    "        epsilon (float): Small constant added to the diagonal.\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: Linear SCM output z_l, shape [B, n].\n",
    "    \"\"\"\n",
    "    n = A.size(0)\n",
    "    I = torch.eye(n, device=A.device, dtype=A.dtype)\n",
    "    M = I - A.t() + epsilon * I\n",
    "    # Use pseudo-inverse to handle singularity\n",
    "    C_inv = torch.linalg.pinv(M)\n",
    "    z_l = torch.matmul(es, C_inv.t())\n",
    "    return z_l\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dea7713",
   "metadata": {
    "papermill": {
     "duration": 0.00716,
     "end_time": "2025-04-26T13:46:25.984811",
     "exception": false,
     "start_time": "2025-04-26T13:46:25.977651",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Non-Linearity Through NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d08c7e86",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-26T13:46:26.000587Z",
     "iopub.status.busy": "2025-04-26T13:46:26.000291Z",
     "iopub.status.idle": "2025-04-26T13:46:26.004791Z",
     "shell.execute_reply": "2025-04-26T13:46:26.003968Z"
    },
    "papermill": {
     "duration": 0.013921,
     "end_time": "2025-04-26T13:46:26.006079",
     "exception": false,
     "start_time": "2025-04-26T13:46:25.992158",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NonlinearSCM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_dim (int): Dimension of the input zₛ from the linear SCM.\n",
    "            hidden_dim (int): Number of neurons in the hidden layer.\n",
    "            output_dim (int): Desired output dimension (typically same as input_dim).\n",
    "        \"\"\"\n",
    "        super(NonlinearSCM, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc159645",
   "metadata": {
    "papermill": {
     "duration": 0.006871,
     "end_time": "2025-04-26T13:46:26.020190",
     "exception": false,
     "start_time": "2025-04-26T13:46:26.013319",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Merging Zc and Er"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3ef597d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-26T13:46:26.035036Z",
     "iopub.status.busy": "2025-04-26T13:46:26.034821Z",
     "iopub.status.idle": "2025-04-26T13:46:26.038120Z",
     "shell.execute_reply": "2025-04-26T13:46:26.037451Z"
    },
    "papermill": {
     "duration": 0.012147,
     "end_time": "2025-04-26T13:46:26.039440",
     "exception": false,
     "start_time": "2025-04-26T13:46:26.027293",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def merge_latent(z_causal, er):\n",
    "    \"\"\"\n",
    "    Merges the causal representation and residual latent information.\n",
    "    \n",
    "    Args:\n",
    "        z_causal (torch.Tensor): Nonlinear causal representation, shape [B, n]\n",
    "        er (torch.Tensor): Residual latent information, shape [B, m]\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: Merged latent representation, shape [B, n+m]\n",
    "    \"\"\"\n",
    "    return torch.cat([z_causal, er], dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f6c241",
   "metadata": {
    "papermill": {
     "duration": 0.006928,
     "end_time": "2025-04-26T13:46:26.053542",
     "exception": false,
     "start_time": "2025-04-26T13:46:26.046614",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Merging Through MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cabe1bc6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-26T13:46:26.068657Z",
     "iopub.status.busy": "2025-04-26T13:46:26.068451Z",
     "iopub.status.idle": "2025-04-26T13:46:26.072648Z",
     "shell.execute_reply": "2025-04-26T13:46:26.072077Z"
    },
    "papermill": {
     "duration": 0.013144,
     "end_time": "2025-04-26T13:46:26.073806",
     "exception": false,
     "start_time": "2025-04-26T13:46:26.060662",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LearnedFusion(nn.Module):\n",
    "    \"\"\"\n",
    "    A fusion network that learns to combine the causal representation\n",
    "    (after SCM transformation, z_l) and the residual latent factors (er)\n",
    "    into a final latent representation.\n",
    "    \"\"\"\n",
    "    def __init__(self, es_dim, er_dim, output_dim):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            es_dim (int): Dimension of the causal part (from SCM), e.g. 4.\n",
    "            er_dim (int): Dimension of the residual part, e.g. (z_dim - 4).\n",
    "            output_dim (int): Final latent dimension (usually equal to z_dim).\n",
    "        \"\"\"\n",
    "        super(LearnedFusion, self).__init__()\n",
    "        self.fc1 = nn.Linear(es_dim + er_dim, output_dim)\n",
    "        self.fc2 = nn.Linear(output_dim, output_dim)\n",
    "    \n",
    "    def forward(self, z_l, er):\n",
    "        # Concatenate the SCM-transformed causal part and residual part\n",
    "        x = torch.cat([z_l, er], dim=1)\n",
    "        x = F.gelu(self.fc1(x))\n",
    "        z_fused = self.fc2(x)\n",
    "        return z_fused"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee877a37",
   "metadata": {
    "papermill": {
     "duration": 0.006831,
     "end_time": "2025-04-26T13:46:26.087640",
     "exception": false,
     "start_time": "2025-04-26T13:46:26.080809",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Conv Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b8092d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-26T13:46:26.102504Z",
     "iopub.status.busy": "2025-04-26T13:46:26.102286Z",
     "iopub.status.idle": "2025-04-26T13:46:26.109270Z",
     "shell.execute_reply": "2025-04-26T13:46:26.108587Z"
    },
    "papermill": {
     "duration": 0.015832,
     "end_time": "2025-04-26T13:46:26.110446",
     "exception": false,
     "start_time": "2025-04-26T13:46:26.094614",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ConvDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolutional Decoder for reconstructing 64x64x3 images from a latent vector.\n",
    "    Designed to be roughly the inverse of the provided ConvEncoder's 'conv6' path.\n",
    "\n",
    "    Input: Latent vector z, shape [B, z_dim]\n",
    "    Output: Reconstructed image, shape [B, 3, 64, 64] (values typically in [0, 1] via Sigmoid)\n",
    "    \"\"\"\n",
    "    def __init__(self, z_dim=64, target_channels=3):\n",
    "        super().__init__()\n",
    "        self.z_dim = z_dim\n",
    "        self.target_channels = target_channels # Should be 3 for RGB images\n",
    "\n",
    "        # Calculate the number of channels needed before the final 1x1 conv in the encoder\n",
    "        # This was 256 in the encoder's conv6\n",
    "        initial_decoder_channels = 256\n",
    "\n",
    "        # 1. Initial dense layer to project z and reshape\n",
    "        # Projects z_dim to the shape needed to start the transposed conv sequence (B x 256 x 1 x 1)\n",
    "        self.fc = nn.Linear(z_dim, initial_decoder_channels * 1 * 1)\n",
    "\n",
    "        # 2. Transposed Convolutional Layers (roughly mirroring encoder's conv6 in reverse)\n",
    "        self.deconv_layers = nn.Sequential(\n",
    "            # Input: B x 256 x 1 x 1\n",
    "            nn.ReLU(True),\n",
    "            # Reverse Conv2d(64, 256, 4, 1, padding=1) -> Output: B x 64 x 2 x 2\n",
    "            nn.ConvTranspose2d(initial_decoder_channels, 64, 4, 1, 1), # k=4, s=1, p=1 -> H_out = (1-1)*1 - 2*1 + 4 = 2\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            # Reverse Conv2d(64, 64, 4, 2, 1) -> Output: B x 64 x 4 x 4\n",
    "            nn.ConvTranspose2d(64, 64, 4, 2, 1), # k=4, s=2, p=1 -> H_out = (2-1)*2 - 2*1 + 4 = 4\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            # Reverse Conv2d(64, 64, 4, 2, 1) -> Output: B x 64 x 8 x 8\n",
    "            nn.ConvTranspose2d(64, 64, 4, 2, 1), # k=4, s=2, p=1 -> H_out = (4-1)*2 - 2*1 + 4 = 8\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            # Reverse Conv2d(32, 64, 4, 2, 1) -> Output: B x 32 x 16 x 16\n",
    "            nn.ConvTranspose2d(64, 32, 4, 2, 1), # k=4, s=2, p=1 -> H_out = (8-1)*2 - 2*1 + 4 = 16\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            # Reverse Conv2d(32, 32, 4, 2, 1) -> Output: B x 32 x 32 x 32\n",
    "            nn.ConvTranspose2d(32, 32, 4, 2, 1), # k=4, s=2, p=1 -> H_out = (16-1)*2 - 2*1 + 4 = 32\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            # Reverse Conv2d(3, 32, 4, 2, 1) -> Output: B x 3 x 64 x 64\n",
    "            nn.ConvTranspose2d(32, self.target_channels, 4, 2, 1) # k=4, s=2, p=1 -> H_out = (32-1)*2 - 2*1 + 4 = 64\n",
    "        )\n",
    "\n",
    "        print(f\"ConvDecoder initialized for z_dim = {self.z_dim}\")\n",
    "        print(f\"Expected input shape: [B, {self.z_dim}]\")\n",
    "        print(f\"Output shape: [B, {self.target_channels}, 64, 64]\")\n",
    "\n",
    "    def forward(self, z):\n",
    "        \"\"\"\n",
    "        Decodes the latent vector z into an image.\n",
    "\n",
    "        Args:\n",
    "            z (torch.Tensor): Latent vector batch, shape [B, z_dim].\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Reconstructed image batch, shape [B, 3, 64, 64].\n",
    "                          Values are mapped to [0, 1] by the final Sigmoid activation.\n",
    "        \"\"\"\n",
    "        # 1. Project and reshape\n",
    "        # Ensure z is flat [B, z_dim]\n",
    "        z_flat = z.view(z.shape[0], -1)\n",
    "        if z_flat.shape[1] != self.z_dim:\n",
    "             raise ValueError(f\"Input z dimension ({z_flat.shape[1]}) does not match decoder's expected z_dim ({self.z_dim})\")\n",
    "\n",
    "        x = self.fc(z_flat)\n",
    "        # Reshape to [B, Channels, Height, Width] for convolutional layers\n",
    "        x = x.view(x.shape[0], 256, 1, 1)\n",
    "\n",
    "        # 2. Pass through transposed convolutional layers\n",
    "        x = self.deconv_layers(x)\n",
    "\n",
    "        # 3. Apply final activation function (Sigmoid for output in [0, 1])\n",
    "        # If your images are normalized differently (e.g., [-1, 1]), use torch.tanh instead.\n",
    "        reconstruction = torch.sigmoid(x)\n",
    "\n",
    "        return reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a92db9b",
   "metadata": {
    "papermill": {
     "duration": 0.007188,
     "end_time": "2025-04-26T13:46:26.125743",
     "exception": false,
     "start_time": "2025-04-26T13:46:26.118555",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Resnet_Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ecbfef80",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-26T13:46:26.141435Z",
     "iopub.status.busy": "2025-04-26T13:46:26.141104Z",
     "iopub.status.idle": "2025-04-26T13:46:26.157058Z",
     "shell.execute_reply": "2025-04-26T13:46:26.156291Z"
    },
    "papermill": {
     "duration": 0.025905,
     "end_time": "2025-04-26T13:46:26.158888",
     "exception": false,
     "start_time": "2025-04-26T13:46:26.132983",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class UpsampleResBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A Residual Block for Upsampling using ConvTranspose2d.\n",
    "    Applies BN and ReLU before convolutions (pre-activation style).\n",
    "    Handles changes in channels and spatial dimensions via the skip connection.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, stride=2, kernel_size=4, padding=1, output_padding=0):\n",
    "        super().__init__()\n",
    "        self.stride = stride\n",
    "        self.channels_match = (in_channels == out_channels)\n",
    "\n",
    "        # Main (Residual) Path - F(x)\n",
    "        self.bn1 = nn.BatchNorm2d(in_channels)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        # Upsamples spatially if stride > 1, also changes channels\n",
    "        self.conv_t1 = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=kernel_size,\n",
    "                                          stride=stride, padding=padding, output_padding=output_padding, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        # Second conv (kernel 3x3) refines features without changing size/channels\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "\n",
    "        # Skip Connection Path\n",
    "        self.needs_skip_proj = (stride > 1) or not self.channels_match\n",
    "        if self.needs_skip_proj:\n",
    "            # This ConvTranspose handles both spatial upsampling (if stride>1)\n",
    "            # and channel matching needed for the addition.\n",
    "            # Using kernel=1 for stride=1 upsampling essentially becomes a 1x1 conv for channels.\n",
    "            # For stride=2, a kernel > 1 (like 2 or 4) might be needed depending on desired output alignment.\n",
    "            # Let's use a simpler approach: Upsample then 1x1 Conv if needed.\n",
    "            self.skip_upsample = nn.Identity() # Default if only channels change\n",
    "            if stride > 1:\n",
    "                 # Use interpolation + 1x1 conv for clarity and flexibility\n",
    "                 self.skip_upsample = nn.Upsample(scale_factor=stride, mode='nearest') # Or 'bilinear'\n",
    "                 # If only channels change (stride=1, C_in!=C_out), skip_in_channels=in_channels\n",
    "                 # If only stride changes (stride>1, C_in==C_out), skip_in_channels=in_channels\n",
    "                 # If both change, skip_in_channels=in_channels\n",
    "                 skip_in_channels = in_channels\n",
    "            elif not self.channels_match: # stride is 1, only channels change\n",
    "                 skip_in_channels = in_channels\n",
    "            else: # stride = 1, channels match = identity skip\n",
    "                 skip_in_channels = in_channels # Value not used but set for consistency\n",
    "\n",
    "            # 1x1 Conv to adjust channels after upsampling (if needed) or if only channels changed\n",
    "            self.skip_conv = nn.Conv2d(skip_in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # --- Skip Connection Path ---\n",
    "        identity = x\n",
    "        if self.needs_skip_proj:\n",
    "            identity = self.skip_upsample(identity) # Upsample spatially if needed\n",
    "            identity = self.skip_conv(identity)   # Adjust channels if needed\n",
    "\n",
    "        # --- Main Path ---\n",
    "        out = self.bn1(x)\n",
    "        out = self.relu1(out)\n",
    "        out = self.conv_t1(out) # Upsample and change channels\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.conv2(out) # Refine features\n",
    "\n",
    "        # --- Add Skip Connection ---\n",
    "        out = out + identity # Element-wise addition\n",
    "\n",
    "        return out\n",
    "\n",
    "class ConvDecoderResNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolutional Decoder using UpsampleResBlocks.\n",
    "\n",
    "    Input: Latent vector z, shape [B, z_dim]\n",
    "    Output: Reconstructed image logits, shape [B, 3, 64, 64]\n",
    "    \"\"\"\n",
    "    def __init__(self, z_dim=64, target_channels=3, start_channels=256, blocks_per_stage=[1,1,1,1,1,1]):\n",
    "        super().__init__()\n",
    "        self.z_dim = z_dim\n",
    "        self.target_channels = target_channels\n",
    "        self.start_channels = start_channels # Channels after initial dense layer\n",
    "\n",
    "        # 1. Initial dense layer and reshape\n",
    "        self.fc = nn.Linear(z_dim, self.start_channels * 1 * 1)\n",
    "\n",
    "        # 2. Sequence of Upsampling ResBlocks\n",
    "        layers = []\n",
    "        current_channels = self.start_channels\n",
    "        # Define channel progression (example, customize as needed)\n",
    "        channel_config = [256, 128, 64, 32, 16] # Channels *after* each stage\n",
    "\n",
    "        # Stage 1: 1x1 -> 2x2 (Special stride/padding)\n",
    "        out_ch = channel_config[0]\n",
    "        layers.append(UpsampleResBlock(current_channels, out_ch, stride=2, kernel_size=4, padding=1, output_padding=0)) # 1->2 requires careful params or adjustment\n",
    "        # For stride=2: output = (input-1)*stride + kernel - 2*padding + output_padding\n",
    "        # Output for k=4,s=2,p=1,op=0: (1-1)*2 + 4 - 2*1 + 0 = 2. Seems okay.\n",
    "        current_channels = out_ch\n",
    "\n",
    "        # Stage 2: 2x2 -> 4x4\n",
    "        out_ch = channel_config[0] # Keep 256\n",
    "        layers.append(UpsampleResBlock(current_channels, out_ch, stride=2, kernel_size=4, padding=1))\n",
    "        current_channels = out_ch\n",
    "\n",
    "        # Stage 3: 4x4 -> 8x8\n",
    "        out_ch = channel_config[1] # 128\n",
    "        layers.append(UpsampleResBlock(current_channels, out_ch, stride=2, kernel_size=4, padding=1))\n",
    "        current_channels = out_ch\n",
    "\n",
    "        # Stage 4: 8x8 -> 16x16\n",
    "        out_ch = channel_config[2] # 64\n",
    "        layers.append(UpsampleResBlock(current_channels, out_ch, stride=2, kernel_size=4, padding=1))\n",
    "        current_channels = out_ch\n",
    "\n",
    "        # Stage 5: 16x16 -> 32x32\n",
    "        out_ch = channel_config[3] # 32\n",
    "        layers.append(UpsampleResBlock(current_channels, out_ch, stride=2, kernel_size=4, padding=1))\n",
    "        current_channels = out_ch\n",
    "\n",
    "        # Stage 6: 32x32 -> 64x64\n",
    "        out_ch = channel_config[4] # 16\n",
    "        layers.append(UpsampleResBlock(current_channels, out_ch, stride=2, kernel_size=4, padding=1))\n",
    "        current_channels = out_ch\n",
    "\n",
    "        self.res_blocks = nn.Sequential(*layers)\n",
    "\n",
    "        # 3. Final layers to get to target channels\n",
    "        self.final_bn = nn.BatchNorm2d(current_channels)\n",
    "        self.final_relu = nn.ReLU(inplace=True)\n",
    "        # Use a standard 3x3 conv for final feature mapping\n",
    "        self.final_conv = nn.Conv2d(current_channels, self.target_channels, kernel_size=3, stride=1, padding=1, bias=True) # Can use bias here\n",
    "\n",
    "        print(f\"ConvDecoderResNet initialized for z_dim = {self.z_dim}\")\n",
    "        print(f\"Upsampling blocks built.\")\n",
    "        print(f\"Output shape (logits): [B, {self.target_channels}, 64, 64]\")\n",
    "\n",
    "\n",
    "    def forward(self, z):\n",
    "        \"\"\"\n",
    "        Decodes the latent vector z into image logits.\n",
    "        \"\"\"\n",
    "        # 1. Project and reshape\n",
    "        z_flat = z.view(z.shape[0], -1)\n",
    "        if z_flat.shape[1] != self.z_dim:\n",
    "            raise ValueError(f\"Input z dimension ({z_flat.shape[1]}) does not match decoder's expected z_dim ({self.z_dim})\")\n",
    "\n",
    "        x = self.fc(z_flat)\n",
    "        x = x.view(x.shape[0], self.start_channels, 1, 1) # Reshape B x C x 1 x 1\n",
    "\n",
    "        # 2. Pass through Residual Blocks\n",
    "        x = self.res_blocks(x) # Output shape B x final_block_channels x 64 x 64\n",
    "\n",
    "        # 3. Apply final BN, ReLU, and Conv\n",
    "        x = self.final_bn(x)\n",
    "        x = self.final_relu(x)\n",
    "        logits = self.final_conv(x) # Final layer, no activation\n",
    "\n",
    "        # Return LOGITS\n",
    "        return logits\n",
    "\n",
    "\n",
    "# --- Example Usage and Notes ---\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# z_dim = 64\n",
    "# target_channels = 3\n",
    "\n",
    "# # Instantiate\n",
    "# decoder = ConvDecoderResNet(z_dim=z_dim, target_channels=target_channels).to(device)\n",
    "\n",
    "# # Create dummy input\n",
    "# dummy_z = torch.randn(4, z_dim).to(device) # Batch size 4\n",
    "\n",
    "# # Forward pass\n",
    "# with torch.no_grad():\n",
    "#     output_logits = decoder(dummy_z)\n",
    "\n",
    "# print(f\"Output logits shape: {output_logits.shape}\") # Should be [4, 3, 64, 64]\n",
    "\n",
    "# # Remember to apply sigmoid OUTSIDE for visualization or standard BCE loss\n",
    "# output_image = torch.sigmoid(output_logits)\n",
    "# print(f\"Output image shape after sigmoid: {output_image.shape}\")\n",
    "\n",
    "# # Recommended Loss:\n",
    "# # L_rec = F.binary_cross_entropy_with_logits(output_logits, target_images, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b38482",
   "metadata": {
    "papermill": {
     "duration": 0.007813,
     "end_time": "2025-04-26T13:46:26.174996",
     "exception": false,
     "start_time": "2025-04-26T13:46:26.167183",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# loss_functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc27b322",
   "metadata": {
    "papermill": {
     "duration": 0.007179,
     "end_time": "2025-04-26T13:46:26.189719",
     "exception": false,
     "start_time": "2025-04-26T13:46:26.182540",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# KL_divergence_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e78bbd30",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-26T13:46:26.205819Z",
     "iopub.status.busy": "2025-04-26T13:46:26.205493Z",
     "iopub.status.idle": "2025-04-26T13:46:26.210473Z",
     "shell.execute_reply": "2025-04-26T13:46:26.209678Z"
    },
    "papermill": {
     "duration": 0.014685,
     "end_time": "2025-04-26T13:46:26.211736",
     "exception": false,
     "start_time": "2025-04-26T13:46:26.197051",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def kl_divergence_conditional(mu, logvar, u):\n",
    "    \"\"\"\n",
    "    Computes the KL divergence between q(z|x,u) ~ N(mu, sigma^2)\n",
    "    and the conditional prior p(z|u) ~ N(u, I).\n",
    "    \n",
    "    Args:\n",
    "        mu (torch.Tensor): Mean from the encoder, shape [B, z_dim].\n",
    "        logvar (torch.Tensor): Log variance from the encoder, shape [B, z_dim].\n",
    "        u (torch.Tensor): Conditional prior mean (e.g., from annotations), shape [B, z_dim].\n",
    "                        If u originally has lower dimension, you must map it to z_dim.\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: KL divergence loss averaged over the batch.\n",
    "    \"\"\"\n",
    "    # Convert logvar to variance\n",
    "    sigma_sq = torch.exp(logvar)\n",
    "    # Compute KL divergence per latent dimension:\n",
    "    # KL = 0.5 * [sigma_sq + (mu - u)^2 - 1 - logvar]\n",
    "    kl = 0.5 * torch.sum(sigma_sq + (mu - u)**2 - 1 - logvar, dim=1)\n",
    "    return torch.mean(kl)\n",
    "\n",
    "\n",
    "\n",
    "def kl_Divergence_conditional(mu_q, log_var_q, mu_p, log_var_p):\n",
    "    var_q = log_var_q.exp()\n",
    "    var_p = log_var_p.exp()\n",
    "    kl = 0.5 * (\n",
    "        (var_q / var_p) + \n",
    "        ((mu_p - mu_q).pow(2) / var_p) - \n",
    "        1 + \n",
    "        log_var_p - log_var_q\n",
    "    ).sum(1)\n",
    "    return kl.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7533fe5",
   "metadata": {
    "papermill": {
     "duration": 0.007101,
     "end_time": "2025-04-26T13:46:26.226204",
     "exception": false,
     "start_time": "2025-04-26T13:46:26.219103",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Reconstruction_Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "400b8e14",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-26T13:46:26.241707Z",
     "iopub.status.busy": "2025-04-26T13:46:26.241437Z",
     "iopub.status.idle": "2025-04-26T13:46:26.244897Z",
     "shell.execute_reply": "2025-04-26T13:46:26.244296Z"
    },
    "papermill": {
     "duration": 0.012645,
     "end_time": "2025-04-26T13:46:26.246115",
     "exception": false,
     "start_time": "2025-04-26T13:46:26.233470",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reconstruction_loss1(x, x_reconstructed):\n",
    "    \"\"\"\n",
    "    Computes the reconstruction loss between the original images and the reconstructed images.\n",
    "    \n",
    "    Args:\n",
    "        x (torch.Tensor): Original images with shape [B, C, H, W]. Expected to be in [0, 1].\n",
    "        x_reconstructed (torch.Tensor): Reconstructed images with shape [B, C, H, W]. Expected to be in [0, 1] if using Sigmoid.\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: The scalar reconstruction loss averaged over the batch.\n",
    "    \"\"\"\n",
    "    # Compute Binary Cross-Entropy loss over all pixels and average over the batch.\n",
    "    return F.binary_cross_entropy(x_reconstructed, x, reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b380b514",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-26T13:46:26.261477Z",
     "iopub.status.busy": "2025-04-26T13:46:26.261192Z",
     "iopub.status.idle": "2025-04-26T13:46:26.264872Z",
     "shell.execute_reply": "2025-04-26T13:46:26.264245Z"
    },
    "papermill": {
     "duration": 0.012836,
     "end_time": "2025-04-26T13:46:26.266182",
     "exception": false,
     "start_time": "2025-04-26T13:46:26.253346",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reconstruction_loss(target_images, reconstructed_logits):\n",
    "     \"\"\" Calculates BCE loss between images in [0,1] and decoder logits. \"\"\"\n",
    "     # Ensure target_images are indeed [0,1]\n",
    "     # You might add assert statements here in debug mode\n",
    "     # assert target_images.min() >= 0.0 and target_images.max() <= 1.0\n",
    "\n",
    "     # reduction='mean' averages over all elements (pixels, channels, batch)\n",
    "     # Often VAEs sum over features and average over batch:\n",
    "     loss_per_element = F.binary_cross_entropy_with_logits(reconstructed_logits, target_images, reduction='none')\n",
    "     loss_summed = loss_per_element.view(loss_per_element.size(0), -1).sum(1) # Sum over all dims except batch\n",
    "     return loss_summed.mean() # Average over batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3725b5",
   "metadata": {
    "papermill": {
     "duration": 0.007425,
     "end_time": "2025-04-26T13:46:26.280936",
     "exception": false,
     "start_time": "2025-04-26T13:46:26.273511",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Causal_Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5949f361",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-26T13:46:26.296707Z",
     "iopub.status.busy": "2025-04-26T13:46:26.296410Z",
     "iopub.status.idle": "2025-04-26T13:46:26.300773Z",
     "shell.execute_reply": "2025-04-26T13:46:26.300023Z"
    },
    "papermill": {
     "duration": 0.013605,
     "end_time": "2025-04-26T13:46:26.301976",
     "exception": false,
     "start_time": "2025-04-26T13:46:26.288371",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reweighted_loss(factual_loss, counterfactual_loss, r, k, gamma=1.0):\n",
    "    reweight_factor = (1 - r) / (1 - r ** k)\n",
    "    loss = reweight_factor * (factual_loss - gamma * counterfactual_loss)\n",
    "    return loss\n",
    "def compute_bce_loss(logits, annotations):\n",
    "    \"\"\"\n",
    "    Computes the binary cross-entropy loss (with logits) between logits and annotations.\n",
    "    \n",
    "    Args:\n",
    "        logits (torch.Tensor): Classifier output logits, shape [B, num_attributes].\n",
    "        annotations (torch.Tensor): Ground truth annotations, shape [B, num_attributes]. \n",
    "                                    Expected to be in [0, 1].\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: Scalar loss (mean over the batch).\n",
    "    \"\"\"\n",
    "    loss = F.binary_cross_entropy_with_logits(logits, annotations, reduction='mean')\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86ba5a3",
   "metadata": {
    "papermill": {
     "duration": 0.007232,
     "end_time": "2025-04-26T13:46:26.317021",
     "exception": false,
     "start_time": "2025-04-26T13:46:26.309789",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Scm_Consistency_Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "80445584",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-26T13:46:26.332584Z",
     "iopub.status.busy": "2025-04-26T13:46:26.332286Z",
     "iopub.status.idle": "2025-04-26T13:46:26.336261Z",
     "shell.execute_reply": "2025-04-26T13:46:26.335590Z"
    },
    "papermill": {
     "duration": 0.013137,
     "end_time": "2025-04-26T13:46:26.337453",
     "exception": false,
     "start_time": "2025-04-26T13:46:26.324316",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def scm_consistency_loss(z_s, f_out, eps_s, kappa=0.1):\n",
    "    \"\"\"\n",
    "    Computes the SCM consistency loss with threshold kappa:\n",
    "        L_f = max(0, MSE(z_s, f_out + eps_s) - kappa)\n",
    "    \n",
    "    Args:\n",
    "        z_s (torch.Tensor): Nonlinear causal representation from SCM, shape [B, causal_dim].\n",
    "        f_out (torch.Tensor): Output of applying the nonlinear function f to A^T * z_s, shape [B, causal_dim].\n",
    "        eps_s (torch.Tensor): Exogenous causal factors from the encoder (the causal part), shape [B, causal_dim].\n",
    "        kappa (float): Threshold constant. Loss is zero if MSE is below kappa. Default is 0.1.\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: SCM consistency loss (scalar).\n",
    "    \"\"\"\n",
    "    # Compute the mean squared error between z_s and the sum (f_out + eps_s)\n",
    "    mse = F.mse_loss(z_s, f_out + eps_s, reduction='mean')\n",
    "    # Use a hinge (ReLU) to enforce the threshold: loss is (mse - kappa) if mse > kappa, else 0.\n",
    "    loss = F.relu(mse - kappa)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356c7d9a",
   "metadata": {
    "papermill": {
     "duration": 0.007001,
     "end_time": "2025-04-26T13:46:26.351554",
     "exception": false,
     "start_time": "2025-04-26T13:46:26.344553",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d7c916fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-26T13:46:26.366932Z",
     "iopub.status.busy": "2025-04-26T13:46:26.366698Z",
     "iopub.status.idle": "2025-04-26T13:46:26.377014Z",
     "shell.execute_reply": "2025-04-26T13:46:26.375917Z"
    },
    "papermill": {
     "duration": 0.019434,
     "end_time": "2025-04-26T13:46:26.378118",
     "exception": true,
     "start_time": "2025-04-26T13:46:26.358684",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 158)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m158\u001b[0m\n\u001b[0;31m    print(f\"Epoch {epoch+1}: Total Loss: {total_loss.item():.4f} | L_rec: {L_rec.item():.4f} | \"\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Hyperparameters and device settings\n",
    "num_epochs = 150\n",
    "lr = 1e-5\n",
    "z_dim = 64\n",
    "causal_dim = 4\n",
    "full_annotation_dim = 40      # Full annotation vector dimension for encoder and annotation mapper\n",
    "causal_annotation_dim = 4     # Causal annotation subset for classifier\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Beta annealing schedule for KL divergence loss\n",
    "start_anneal_epoch = 80\n",
    "end_anneal_epoch = 150\n",
    "target_beta_kl = 0.01  # Target beta value for KL loss\n",
    "initial_beta_kl = 0.0001\n",
    "\n",
    "# Instantiate modules with updated dimensions\n",
    "encoder = ConvEncoderConditionalResNet(z_dim_target=z_dim, annotation_dim=full_annotation_dim).to(device)\n",
    "decoder = ConvDecoderResNet(z_dim=z_dim, target_channels=3).to(device)\n",
    "classifier = CausalMLPClassifier(latent_dim=z_dim, annotation_dim=causal_annotation_dim,\n",
    "                                 hidden_dim=256, output_dim=causal_annotation_dim).to(device)\n",
    "nonlinear_scm = NonlinearSCM(input_dim=causal_dim, hidden_dim=16, output_dim=causal_dim).to(device)\n",
    "fusion_module = LearnedFusion(es_dim=causal_dim, er_dim=z_dim - causal_dim, output_dim=z_dim).to(device)\n",
    "annotation_mapper = nn.Linear(full_annotation_dim, z_dim).to(device)\n",
    "\n",
    "# Optimizer: update all parameters together\n",
    "optimizer = optim.Adam(\n",
    "    list(encoder.parameters()) +\n",
    "    list(decoder.parameters()) +\n",
    "    list(classifier.parameters()) +\n",
    "    list(nonlinear_scm.parameters()) +\n",
    "    list(fusion_module.parameters()) +\n",
    "    list(annotation_mapper.parameters()),\n",
    "    lr=lr\n",
    ")\n",
    "\n",
    "# Loss hyperparameters\n",
    "r = 0.5\n",
    "k = 10\n",
    "gamma = 1.0\n",
    "weight_causal = 1.0\n",
    "weight_scm = 1.0\n",
    "kappa = 0.1\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (images, full_annotations, causal_annotations) in enumerate(dataloader):\n",
    "        if images is None or full_annotations is None or causal_annotations is None:\n",
    "            continue\n",
    "\n",
    "        images = images.to(device)                        # [B, 3, 64, 64]\n",
    "        full_annotations = full_annotations.to(device)    # [B, 40]\n",
    "        causal_annotations = causal_annotations.to(device)  # [B, 4]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Encoder: use images and full annotations to get latent parameters\n",
    "        mu, logvar = encoder(images, full_annotations)    # [B, 64]\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        z = mu + eps * std                                  # [B, 64]\n",
    "\n",
    "        # Partition z into causal part (Es) and residual (Er)\n",
    "        es = z[:, :causal_dim]                              # [B, 4]\n",
    "        er = z[:, causal_dim:]                              # [B, 60] if z_dim=64\n",
    "\n",
    "        # Compute causal matrix A using classifier outputs; classifier uses causal_annotations (4-dim)\n",
    "        A_est = compute_causal_matrix(z, causal_annotations, classifier)  # [4, 4]\n",
    "        if epoch == 0 and batch_idx == 0:\n",
    "            print(f\"\\n--- A_estimated (Epoch: {epoch+1}, Batch: {batch_idx}) ---\")\n",
    "            print(A_est)\n",
    "\n",
    "        if torch.isnan(A_est).any() or torch.isinf(A_est).any():\n",
    "            raise ValueError(\"A_estimated contains non-finite values\")\n",
    "\n",
    "        # Linear SCM: transform Es using A_est\n",
    "        z_l = linear_scm(es, A_est)                         # [B, 4]\n",
    "\n",
    "        # Nonlinear SCM: map z_l to a nonlinear causal representation z_s\n",
    "        z_s = nonlinear_scm(z_l)                            # [B, 4]\n",
    "\n",
    "        # Fusion: combine z_s and Er via the fusion module to get final latent representation z_c\n",
    "        z_c = fusion_module(z_s, er)                        # [B, 64]\n",
    "\n",
    "        # Decoder: reconstruct images from z_c\n",
    "        x_reconstructed_logits = decoder(z_c)# [B, 3, 64, 64]\n",
    "\n",
    "        x_reconstructed = torch.sigmoid(x_reconstructed_logits)\n",
    "        # Compute Reconstruction Loss\n",
    "        L_rec = reconstruction_loss1(images, x_reconstructed)\n",
    "\n",
    "        # For KL divergence, map full annotations (40 dims) to latent dimension using annotation_mapper.\n",
    "        u_mapped = annotation_mapper(full_annotations)      # [B, 64]\n",
    "        L_kl = kl_divergence_conditional(mu, logvar, u_mapped)\n",
    "\n",
    "        # Compute Causal Loss:\n",
    "        # Factual: classifier with (z, causal_annotations)\n",
    "        factual_logits_full = classifier(z, causal_annotations)  # [B,4]\n",
    "\n",
    "        L_causal_terms = []\n",
    "        for j in range(causal_annotation_dim):\n",
    "           # isolate uj and its factual logits\n",
    "          uj = causal_annotations[:, j]              # [B]\n",
    "          factual_logits_j = factual_logits_full[:, j]# [B]\n",
    "          factual_loss_j = F.binary_cross_entropy_with_logits(factual_logits_j, uj, reduction='mean')\n",
    "\n",
    "               # build counterfactual losses for this target j\n",
    "    cf_losses_j = []\n",
    "    for i in range(causal_annotation_dim):\n",
    "        u_cf = causal_annotations.clone()\n",
    "        u_cf[:, i] = 0.0\n",
    "        cf_logits_full = classifier(z, u_cf)       # [B,4]\n",
    "        cf_logits_i_j = cf_logits_full[:, j]       # [B]\n",
    "        cf_loss_i_j = F.binary_cross_entropy_with_logits(cf_logits_i_j, uj, reduction='mean')\n",
    "        cf_losses_j.append(cf_loss_i_j)\n",
    "    counterfactual_loss_j = sum(cf_losses_j) / causal_annotation_dim\n",
    "\n",
    "    # compute k_j (number of positives of uj in this batch)\n",
    "    k_j = (uj == 1).sum().item()\n",
    "    epsilon_k = 1e-8\n",
    "    denom = 1 - r**(k_j + epsilon_k)\n",
    "    if abs(denom) < epsilon_k:\n",
    "        factor_j = 1.0\n",
    "    else:\n",
    "        factor_j = (1 - r) / denom\n",
    "\n",
    "    # one causal‐loss term per j\n",
    "    L_causal_terms.append(factor_j * (factual_loss_j - gamma * counterfactual_loss_j))\n",
    "\n",
    "# final causal loss = mean over dimensions\n",
    "L_causal = sum(L_causal_terms) / causal_annotation_dim\n",
    "\n",
    "        # Compute SCM Consistency Loss:\n",
    "        A_transpose = A_est.t()\n",
    "        z_s_transformed = torch.matmul(z_s, A_transpose)     # [B, 4]\n",
    "        f_out = nonlinear_scm(z_s_transformed)               # [B, 4]\n",
    "        L_scm = scm_consistency_loss(z_s, f_out, es, kappa)\n",
    "\n",
    "        # Beta annealing schedule for KL divergence:\n",
    "        if epoch >= start_anneal_epoch:\n",
    "            progress = (epoch - start_anneal_epoch) / (end_anneal_epoch - start_anneal_epoch)\n",
    "            current_beta_kl = initial_beta_kl + progress * (target_beta_kl - initial_beta_kl)\n",
    "            current_beta_kl = min(target_beta_kl, current_beta_kl)\n",
    "        else:\n",
    "            current_beta_kl = initial_beta_kl\n",
    "\n",
    "        # Total Loss with annealed beta for KL divergence\n",
    "        total_loss = L_rec + current_beta_kl * L_kl + weight_causal * L_causal + weight_scm * L_scm\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Total Loss: {total_loss.item():.4f} | L_rec: {L_rec.item():.4f} | \"\n",
    "          f\"L_kl: {L_kl.item():.4f} | L_causal: {L_causal.item():.4f} | L_scm: {L_scm.item():.4f}\")\n",
    "\n",
    "    # GPU memory usage tracking at the end of each epoch\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated(0) / 1024**2  # Convert to MB\n",
    "        reserved = torch.cuda.memory_reserved(0) / 1024**2\n",
    "        print(f\"[GPU] Memory Allocated: {allocated:.2f} MB | Memory Reserved: {reserved:.2f} MB\")\n",
    "\n",
    "    # Display 5 sample original and reconstructed images inline using matplotlib\n",
    "    num_samples_to_display = 5\n",
    "    orig_samples = images[:num_samples_to_display].detach().cpu()\n",
    "    recon_samples = x_reconstructed[:num_samples_to_display].detach().cpu()\n",
    "\n",
    "    grid_orig = make_grid(orig_samples, nrow=num_samples_to_display, normalize=True, scale_each=True)\n",
    "    grid_recon = make_grid(recon_samples, nrow=num_samples_to_display, normalize=True, scale_each=True)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title(\"Original Images\")\n",
    "    plt.imshow(grid_orig.permute(1, 2, 0).numpy())\n",
    "    plt.axis(\"off\")\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title(\"Reconstructed Images\")\n",
    "    plt.imshow(grid_recon.permute(1, 2, 0).numpy())\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f162e8",
   "metadata": {
    "execution": {
     "execution_failed": "2025-04-24T17:28:20.616Z",
     "iopub.execute_input": "2025-04-24T16:53:29.767535Z",
     "iopub.status.busy": "2025-04-24T16:53:29.767248Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Hyperparameters and device settings\n",
    "num_epochs = 150\n",
    "lr = 1e-5\n",
    "z_dim = 64\n",
    "causal_dim = 4\n",
    "full_annotation_dim = 40      # Full annotation vector dimension for encoder and annotation mapper\n",
    "causal_annotation_dim = 4     # Causal annotation subset for classifier\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Beta annealing schedule for KL divergence loss\n",
    "start_anneal_epoch = 80\n",
    "end_anneal_epoch = 150\n",
    "target_beta_kl = 0.01  # Target beta value for KL loss\n",
    "initial_beta_kl = 0.0001\n",
    "\n",
    "# Instantiate modules with updated dimensions\n",
    "encoder = ConvEncoderConditionalResNet(z_dim_target=z_dim, annotation_dim=full_annotation_dim).to(device)\n",
    "decoder = ConvDecoderResNet(z_dim=z_dim, target_channels=3).to(device)\n",
    "classifier = CausalMLPClassifier(latent_dim=z_dim, annotation_dim=causal_annotation_dim,\n",
    "                                 hidden_dim=256, output_dim=causal_annotation_dim).to(device)\n",
    "nonlinear_scm = NonlinearSCM(input_dim=causal_dim, hidden_dim=16, output_dim=causal_dim).to(device)\n",
    "fusion_module = LearnedFusion(es_dim=causal_dim, er_dim=z_dim - causal_dim, output_dim=z_dim).to(device)\n",
    "annotation_mapper = nn.Linear(full_annotation_dim, z_dim).to(device)\n",
    "\n",
    "# Optimizer: update all parameters together\n",
    "optimizer = optim.Adam(\n",
    "    list(encoder.parameters()) +\n",
    "    list(decoder.parameters()) +\n",
    "    list(classifier.parameters()) +\n",
    "    list(nonlinear_scm.parameters()) +\n",
    "    list(fusion_module.parameters()) +\n",
    "    list(annotation_mapper.parameters()),\n",
    "    lr=lr\n",
    ")\n",
    "\n",
    "# Loss hyperparameters\n",
    "r = 0.5\n",
    "k = 10\n",
    "gamma = 1.0\n",
    "weight_causal = 1.0\n",
    "weight_scm = 1.0\n",
    "kappa = 0.1\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (images, full_annotations, causal_annotations) in enumerate(dataloader):\n",
    "        if images is None or full_annotations is None or causal_annotations is None:\n",
    "            continue\n",
    "\n",
    "        images = images.to(device)                        # [B, 3, 64, 64]\n",
    "        full_annotations = full_annotations.to(device)    # [B, 40]\n",
    "        causal_annotations = causal_annotations.to(device)  # [B, 4]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Encoder: use images and full annotations to get latent parameters\n",
    "        mu, logvar = encoder(images, full_annotations)    # [B, 64]\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        z = mu + eps * std                                  # [B, 64]\n",
    "\n",
    "        # Partition z into causal part (Es) and residual (Er)\n",
    "        es = z[:, :causal_dim]                              # [B, 4]\n",
    "        er = z[:, causal_dim:]                              # [B, 60] if z_dim=64\n",
    "\n",
    "        # Compute causal matrix A using classifier outputs; classifier uses causal_annotations (4-dim)\n",
    "        A_est = compute_causal_matrix(z, causal_annotations, classifier)  # [4, 4]\n",
    "        if epoch == 0 and batch_idx == 0:\n",
    "            print(f\"\\n--- A_estimated (Epoch: {epoch+1}, Batch: {batch_idx}) ---\")\n",
    "            print(A_est)\n",
    "\n",
    "        if torch.isnan(A_est).any() or torch.isinf(A_est).any():\n",
    "            raise ValueError(\"A_estimated contains non-finite values\")\n",
    "\n",
    "        # Linear SCM: transform Es using A_est\n",
    "        z_l = linear_scm(es, A_est)                         # [B, 4]\n",
    "\n",
    "        # Nonlinear SCM: map z_l to a nonlinear causal representation z_s\n",
    "        z_s = nonlinear_scm(z_l)                            # [B, 4]\n",
    "\n",
    "        # Fusion: combine z_s and Er via the fusion module to get final latent representation z_c\n",
    "        z_c = fusion_module(z_s, er)                        # [B, 64]\n",
    "\n",
    "        # Decoder: reconstruct images from z_c\n",
    "        x_reconstructed_logits = decoder(z_c)# [B, 3, 64, 64]\n",
    "\n",
    "        x_reconstructed = torch.sigmoid(x_reconstructed_logits)\n",
    "        # Compute Reconstruction Loss\n",
    "        L_rec = reconstruction_loss1(images, x_reconstructed)\n",
    "\n",
    "        # For KL divergence, map full annotations (40 dims) to latent dimension using annotation_mapper.\n",
    "        u_mapped = annotation_mapper(full_annotations)      # [B, 64]\n",
    "        L_kl = kl_divergence_conditional(mu, logvar, u_mapped)\n",
    "        \n",
    "        # Compute Causal Loss:\n",
    "        # Factual: classifier with (z, causal_annotations)\n",
    "        factual_logits = classifier(z, causal_annotations)   # [B, 4]\n",
    "        factual_loss = F.binary_cross_entropy_with_logits(factual_logits, causal_annotations, reduction='mean')\n",
    "        cf_losses = []\n",
    "        for i in range(causal_annotations.shape[1]):\n",
    "            u_cf = causal_annotations.clone()\n",
    "            u_cf[:, i] = 0.0\n",
    "            cf_logits = classifier(z, u_cf)\n",
    "            cf_loss = F.binary_cross_entropy_with_logits(cf_logits, causal_annotations, reduction='mean')\n",
    "            cf_losses.append(cf_loss)\n",
    "        counterfactual_loss = sum(cf_losses) / causal_annotations.shape[1]\n",
    "        L_causal = reweighted_loss(factual_loss, counterfactual_loss, r, k, gamma)\n",
    "\n",
    "        # Compute SCM Consistency Loss:\n",
    "        A_transpose = A_est.t()\n",
    "        z_s_transformed = torch.matmul(z_s, A_transpose)     # [B, 4]\n",
    "        f_out = nonlinear_scm(z_s_transformed)               # [B, 4]\n",
    "        L_scm = scm_consistency_loss(z_s, f_out, es, kappa)\n",
    "\n",
    "        # Beta annealing schedule for KL divergence:\n",
    "        if epoch >= start_anneal_epoch:\n",
    "            progress = (epoch - start_anneal_epoch) / (end_anneal_epoch - start_anneal_epoch)\n",
    "            current_beta_kl = initial_beta_kl + progress * (target_beta_kl - initial_beta_kl)\n",
    "            current_beta_kl = min(target_beta_kl, current_beta_kl)\n",
    "        else:\n",
    "            current_beta_kl = initial_beta_kl\n",
    "\n",
    "        # Total Loss with annealed beta for KL divergence\n",
    "        total_loss = L_rec + current_beta_kl * L_kl + weight_causal * L_causal + weight_scm * L_scm\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Total Loss: {total_loss.item():.4f} | L_rec: {L_rec.item():.4f} | \"\n",
    "          f\"L_kl: {L_kl.item():.4f} | L_causal: {L_causal.item():.4f} | L_scm: {L_scm.item():.4f}\")\n",
    "\n",
    "    # GPU memory usage tracking at the end of each epoch\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated(0) / 1024**2  # Convert to MB\n",
    "        reserved = torch.cuda.memory_reserved(0) / 1024**2\n",
    "        print(f\"[GPU] Memory Allocated: {allocated:.2f} MB | Memory Reserved: {reserved:.2f} MB\")\n",
    "\n",
    "    # Display 5 sample original and reconstructed images inline using matplotlib\n",
    "    num_samples_to_display = 5\n",
    "    orig_samples = images[:num_samples_to_display].detach().cpu()\n",
    "    recon_samples = x_reconstructed[:num_samples_to_display].detach().cpu()\n",
    "\n",
    "    grid_orig = make_grid(orig_samples, nrow=num_samples_to_display, normalize=True, scale_each=True)\n",
    "    grid_recon = make_grid(recon_samples, nrow=num_samples_to_display, normalize=True, scale_each=True)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title(\"Original Images\")\n",
    "    plt.imshow(grid_orig.permute(1, 2, 0).numpy())\n",
    "    plt.axis(\"off\")\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title(\"Reconstructed Images\")\n",
    "    plt.imshow(grid_recon.permute(1, 2, 0).numpy())\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 29561,
     "sourceId": 37705,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7129535,
     "sourceId": 11385760,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 17.796055,
   "end_time": "2025-04-26T13:46:28.802640",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-26T13:46:11.006585",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
